{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_BpdzaIgSzj"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eD6OUkwgSzj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U datasets huggingface_hub fsspec"
      ],
      "metadata": {
        "id": "b35AVJ9c4VOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade --no-cache-dir unsloth"
      ],
      "metadata": {
        "id": "g2b12KTqSEpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel, is_bfloat16_supported, unsloth_train\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import wandb\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "jlPMw2u9GrtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialisation"
      ],
      "metadata": {
        "id": "0lhs_aKCciWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hugging_face_token = userdata.get('HF_TOKEN')\n",
        "wnb_token = userdata.get('WB_TOKEN')\n",
        "\n",
        "# login to WnB\n",
        "wandb.login(key=wnb_token)\n",
        "run = wandb.init(\n",
        "    project='Vikhr-Fine-tuning-1',\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"\n",
        ")"
      ],
      "metadata": {
        "id": "ddssAVKtcDZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 4096 # the maximum sequence length a model can handle\n",
        "dtype = None\n",
        "load_in_4bit = True # 4bit quantization to reduce memory usage\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "# define a system prompt under prompt_style\n",
        "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Ты следишь за качеством работы специалистов клиентской службы. Отвечай на русском языке.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for input, output in zip(inputs, outputs):\n",
        "        text = prompt_style.format(input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "dataset = load_dataset(\"katarinaaaaa/evaluation-of-customer-service\", split = \"train\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get training and validation datasets\n",
        "train_val_split = dataset.train_test_split(test_size=0.1, seed=123)\n",
        "train_dataset = train_val_split['train']\n",
        "validation_dataset = train_val_split['test']\n",
        "\n",
        "train_dataset = train_dataset.map(formatting_prompts_func, batched = True,)\n",
        "validation_dataset = validation_dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "WRcxqwyA4x-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "# apply LoRA (Low-Rank Adaptation) fine-tuning to the model\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, # LoRA rank, determines the size of the trainable adapters (higher = more parameters, lower = more efficiency)\n",
        "    target_modules=[  # list of transformer layers where LoRA adapters will be applied\n",
        "        \"q_proj\",     # query projection in the self-attention mechanism\n",
        "        \"k_proj\",     # key projection in the self-attention mechanism\n",
        "        \"v_proj\",     # value projection in the self-attention mechanism\n",
        "        \"o_proj\",     # output projection from the attention layer\n",
        "        \"gate_proj\",  # used in feed-forward layers (MLP)\n",
        "        \"up_proj\",    # part of the transformer’s feed-forward network (FFN)\n",
        "        \"down_proj\",  # another part of the transformer’s FFN\n",
        "    ],\n",
        "    lora_alpha = 32,  # scaling factor for LoRA updates\n",
        "    lora_dropout = 0, # dropout rate for LoRA layers\n",
        "    bias = \"none\",    # specifies whether LoRA layers should learn bias terms\n",
        "    use_gradient_checkpointing = \"unsloth\", # saves memory by recomputing activations instead of storing them\n",
        "    random_state = 1234, # sets a seed for reproducibility, ensuring the same fine-tuning behavior across runs\n",
        "    use_rslora = False,  # whether to use Rank-Stabilized LoRA\n",
        "    loftq_config = None, # low-bit Fine-Tuning Quantization\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "9gB74G7c2VU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "# initialize the fine-tuning trainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = validation_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2, # uses 2 CPU threads to speed up data preprocessing\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2, # number of examples processed per device (GPU) at a time\n",
        "        gradient_accumulation_steps = 4, # accumulate gradients over 4 steps before updating weights\n",
        "        warmup_steps = 5, # gradually increases learning rate for the first 5 steps\n",
        "        num_train_epochs = 1,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(), # use FP16 if BF16 is not supported to speed up training\n",
        "        bf16 = is_bfloat16_supported(), # use BF16 if supported (better numerical stability on newer GPUs)\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\", # uses memory-efficient AdamW optimizer in 8-bit mode\n",
        "        weight_decay = 0.01, # regularization to prevent overfitting\n",
        "        lr_scheduler_type = \"linear\", # uses a linear learning rate schedule\n",
        "        seed = 1234,\n",
        "        output_dir = \"outputs\",\n",
        "        gradient_checkpointing=True,\n",
        "        do_eval=True,\n",
        "        fp16_full_eval = True,\n",
        "        per_device_eval_batch_size = 2,\n",
        "        eval_accumulation_steps = 4,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=50,\n",
        "        save_total_limit = 1,\n",
        "    ),\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "qmNeWtekhfZp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "# trainer_stats = trainer.train()\n",
        "trainer_stats = unsloth_train(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save w&b statistics\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "gBbVjykzhrVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# convert logs to dataframe\n",
        "logs = trainer.state.log_history\n",
        "df = pd.DataFrame(logs)\n",
        "print(df)\n",
        "\n",
        "# filter only rows with 'loss' key\n",
        "df1 = df[['step', 'epoch', 'loss']].dropna()\n",
        "df2 = df[['step','epoch', 'eval_loss']].dropna()\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(df1['step'], df1['loss'], linestyle='-', label='Training Loss')\n",
        "plt.plot(df2['step'], df2['eval_loss'], linestyle='-', label='Validation Loss')\n",
        "\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# plt.show()\n",
        "plt.savefig('my_plot1.png', dpi=600, transparent=True)"
      ],
      "metadata": {
        "id": "JhPdRTmsZca_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pEuRb1r2Vg"
      },
      "outputs": [],
      "source": [
        "input = \"\"\"Проанализируй предоставленный диалог между клиентом и оператором клиентской службы.\n",
        "Оцени качество работы оператора от 0 до 10 баллов по каждому из следующих критериев:\n",
        "\n",
        "1. Профессионализм и вежливость:\n",
        "- Оператор использует корректность тона, отсутствие грубости или сарказма.\n",
        "- Оператор соблюдает этикет (приветствие, прощание, обращение на «вы» и т.п.).\n",
        "2. Соблюдение регламента и компетентность:\n",
        "- Оператор представился и назвал имя компании.\n",
        "- Оператор разбирается в своей области и корректно отвечает заданные на вопросы.\n",
        "- Оператор заботится о положительном образе компании.\n",
        "3. Эффективность коммуникации:\n",
        "- Ответы оператора четкие и структурированные.\n",
        "- Оператор умеет задавать уточняющие вопросы для понимания проблемы там, где они необходимы.\n",
        "4. Решение проблемы:\n",
        "- Оператор предоставляет корректную информацию.\n",
        "- Оператор стремится предложить решение, даже если запрос сложный.\n",
        "- Оператор предлагает альтернативы там, где это уместно.\n",
        "- Предложенные оператором решения соответствуют общепринятым правилам и закону.\n",
        "- Если обращение связано с ошибкой компании, оператор предлагает дополнительную компенсацию для сохранения лояльности клиента.\n",
        "5. Грамотность речи:\n",
        "- В сообщениях оператора отсутствуют грамматические и пунктуационные ошибки и опечатки.\n",
        "6. Эмпатия и эмоциональный интеллект:\n",
        "- Оператор учитывает эмоциональное состояние клиента (например, использует при необходимости извинения, слова поддержки).\n",
        "- Оператор умеет снизить напряжение в конфликтной ситуации.\n",
        "\n",
        "В ответе представь только json-файл со следующей структурой:\n",
        "\n",
        "{\n",
        "    \"theme\": \"*Тема диалога*\",\n",
        "    \"criteria\":\n",
        "    [\n",
        "        {\n",
        "            \"criterion_name\": \"*Название критерия из списка выше*\",\n",
        "            \"score\": *Оценка указанного критерия (от 0 до 10). Если в диалоге отсутствуют данные для оценки критерия, поставь null*,\n",
        "            \"comments\": \"*Краткое резюме по данному критерию с примерами из диалога, иллюстрирующими выводы*\",\n",
        "        },\n",
        "        {\n",
        "            \"criterion_name\": \"*Название критерия из списка выше*\",\n",
        "            \"score\": *Оценка указанного критерия (от 0 до 10). Если в диалоге отсутствуют данные для оценки критерия, поставь null*,\n",
        "            \"comments\": \"*Краткое резюме по данному критерию с примерами из диалога, иллюстрирующими выводы*\",\n",
        "        },\n",
        "        ...\n",
        "    ],\n",
        "    \"total_score\": *Итоговая средняя оценка по всем указанным критериям*,\n",
        "    \"result\": \"*Решена ли проблема клиента (одно слово Да/Нет)*\",\n",
        "    \"result_comment\": \"*Результат обращения клиента в одном предложении*\",\n",
        "    \"recommendations\": \"*Краткие конкретные рекомендации оператору для повышения качества работы (если есть недочеты). Не используй пункты, пиши сплошным текстом.*\",\n",
        "}\n",
        "\n",
        "Анализ должен быть объективным, без субъективных предположений. Не занижай оценку из-за незначительных недочетов.\n",
        "Не добавляй в ответ ничего лишнего, начни ответ с {, закончи }. Используй только русский язык.\n",
        "\n",
        "Диалог:\n",
        "\n",
        "Оператор: Онлайн-кинотеатр «СинемаЛайф», Дарья. Чем могу помочь?\n",
        "Клиент: Добрый день. Не могу продлить подписку — пишет «Ошибка платежа». Карта точно рабочая.\n",
        "Оператор: Проверила ваш аккаунт: попытка оплаты 15 минут назад отклонена банком. Обновите данные карты в личном кабинете.\n",
        "Клиент: Обновил — та же ошибка. Может, система глючит?\n",
        "Оператор: Попробуйте сократить имя держателя карты: например, Ivanov I.I. вместо полного имени.\n",
        "Клиент: О, сработало! Спасибо!\n",
        "Оператор: Рада помочь! Иногда банки блокируют длинные названия. Подписка активна до 15 ноября.\n",
        "Клиент: А если проблема повторится, куда писать?\n",
        "Оператор: В этот чат или на support@cmnl.ru. Приложите скриншот ошибки.\n",
        "Клиент: Хорошо. Ещё вопрос: почему пропали русские субтитры в новом сериале?\n",
        "Оператор: Технический сбой. Исправим в течение суток. Извините за неудобство!\n",
        "Клиент: Понял. Спасибо за оперативность!\n",
        "Оператор: Всегда рады помочь! Приятного просмотра.\n",
        "\"\"\"\n",
        "\n",
        "FastLanguageModel.for_inference(model) # load the inference model using FastLanguageModel\n",
        "inputs = tokenizer([prompt_style.format(input, \"\",)], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "# use TextStreamer for real-time generating\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs.input_ids, attention_mask = inputs.attention_mask,\n",
        "                   streamer = text_streamer, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving finetuned model"
      ],
      "metadata": {
        "id": "KJGv6C8qjKME"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "### Saving only LoRA adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "if False: model.save_pretrained(\"Vikhr-Customer-Service-Evaluation\") # local saving\n",
        "if False: tokenizer.save_pretrained(\"Vikhr-Customer-Service-Evaluation\")\n",
        "if False: model.push_to_hub(\"katarinaaaaa/Vikhr-Customer-Service-Evaluation\", token=hugging_face_token) # online saving\n",
        "if False: tokenizer.push_to_hub(\"katarinaaaaa/Vikhr-Customer-Service-Evaluation\", token=hugging_face_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 or int4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"Vikhr-Support-Evaluation-2\", tokenizer, save_method = \"merged_16bit\") # local saving\n",
        "model.push_to_hub_merged(\"katarinaaaaa/Vikhr-Customer-Service-Evaluation-2\", tokenizer, save_method = \"merged_16bit\", token = hugging_face_token)\n",
        "\n",
        "# merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"Vikhr-Customer-Service-Evaluation-2\", tokenizer, save_method = \"merged_4bit_forced\",)\n",
        "if False: model.push_to_hub_merged(\"katarinaaaaa/Vikhr-Customer-Service-Evaluation-2\", tokenizer, save_method = \"merged_4bit_forced\", token = hugging_face_token)\n",
        "\n",
        "# just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"Vikhr-Customer-Service-Evaluation-2\", tokenizer, save_method = \"lora\")\n",
        "if False: model.push_to_hub_merged(\"katarinaaaaa/Vikhr-Customer-Service-Evaluation-2\", tokenizer, save_method = \"lora\", token = hugging_face_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# save to 8bit Q8_0 - fast conversion, high resource use, but generally acceptable\n",
        "if False: model.save_pretrained_gguf(\"Vikhr-Customer-Service-Evaluation-GGUF-2\", tokenizer) # local saving\n",
        "if False: model.push_to_hub_gguf(\"katarinaaaaa/Vikhr-Customer-Service-Evaluation-GGUF-2\", tokenizer, token = hugging_face_token)\n",
        "\n",
        "# save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"Vikhr-Customer-Service-Evaluation-GGUF-2\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"katarinaaaaa/Vikhr-Customer-Service-Evaluation-GGUF-2\", tokenizer, quantization_method = \"f16\", token = hugging_face_token)\n",
        "\n",
        "# save to q4_k_m GGUF - uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\n",
        "if False: model.save_pretrained_gguf(\"Vikhr-Customer-Service-Evaluation-GGUF-2\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"katarinaaaaa/Vikhr-Customer-Service-Evaluation-GGUF-2\", tokenizer, quantization_method = \"q4_k_m\", token = hugging_face_token)\n",
        "\n",
        "# save to q4_k_m GGUF - uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\n",
        "if False: model.save_pretrained_gguf(\"Vikhr-Customer-Service-Evaluation-GGUF-2\", tokenizer, quantization_method = \"q5_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"katarinaaaaa/Vikhr-Customer-Service-Evaluation-GGUF-2\", tokenizer, quantization_method = \"q5_k_m\", token = hugging_face_token)\n",
        "\n",
        "# save to multiple GGUF options\n",
        "model.push_to_hub_gguf(\n",
        "    \"katarinaaaaa/Vikhr-Customer-Service-Evaluation-GGUF-2\",\n",
        "    tokenizer,\n",
        "    quantization_method = [\"q8_0\", \"f16\", \"q4_k_m\", \"q5_k_m\"],\n",
        "    token = hugging_face_token,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "uMuVrWbjAzhc",
        "TCv4vXHd61i7"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}